# KAIAK AI in Education Blog Posts: Outlines & Drafts

## How to Use This Document
- Each post includes: Outline → Draft → "Your Input Needed" section
- [INPUT NEEDED] markers show where your authentic experience will strengthen the piece
- Drafts follow the SKILL.md framework (anti-slop, varied structure, MDX components)
- After your input, I'll refine and create final MDX files

---

# POST 1: Academic Integrity

## Title: "The Detection Arms Race Is Over. Here's What to Do Instead."

### Outline

**Hook:** Open with the futility moment—the realization that detection is a losing game

**The Meat:**
- Section 1: The numbers that killed detection-first strategies
- Section 2: Why students cheat (it's not what you think)
- Section 3: The assessment redesign approach
- Section 4: What this looks like in practice

**Bottom Line:** Specific next steps for shifting from detection to design

---

### Draft

Last semester, I watched a colleague spend three hours trying to prove a student used AI on an essay. She ran it through four different detectors. Two said AI. Two said human. The student denied it. The parents demanded evidence. The whole thing ended in a stalemate that satisfied no one.

That's when I realized: we're playing a game we can't win.

[INPUT NEEDED: Do you have a similar story from GGCS or your network? A specific moment when the detection approach failed or felt futile? This opening works but YOUR version would be stronger.]

**The numbers tell the story.** Faculty rate AI-specific plagiarism policies as only 28% effective. Meanwhile, 94% of AI-generated work goes undetected according to University of Reading research. Students who want to evade detection can do so easily—there are entire YouTube channels teaching them how.

The detection tools themselves are unreliable. NPR reported that 73% of student-reported AI detection incidents involve disputed false positives. ESL students get flagged disproportionately. A student in one district was accused of cheating on an essay about music she loved—flagged at 30% probability by a detector—simply because her writing style triggered the algorithm.

Princeton and MIT have advised against relying solely on AI detectors. The Center for Democracy and Technology warns that over-reliance on detection erodes teacher-student trust. We've created an adversarial dynamic where students see teachers as opponents rather than guides.

<Callout type="warning" title="The real cost">
Every hour spent on detection theater is an hour not spent on instruction. Every false accusation damages a relationship that took months to build. Every cat-and-mouse escalation teaches students that the goal is to avoid getting caught—not to learn.
</Callout>

**Why students use AI inappropriately in the first place.** Research points to a combination of factors, and it's rarely because students are "bad" or don't care about honesty.

Time pressure tops the list. Students juggling multiple classes, extracurriculars, and family responsibilities face genuine constraints. When an assignment feels like a box to check rather than an opportunity to learn, the path of least resistance becomes tempting.

Disconnection from purpose matters too. If an assignment feels unengaging or disconnected from their interests and future goals, intrinsic motivation to produce original work diminishes. A student who sees no point in a five-paragraph essay about a book they didn't enjoy will approach it very differently than one writing about something they care about.

Unclear expectations create confusion. When students don't understand what AI use is acceptable versus problematic, they make judgment calls that may not align with teacher expectations. "Don't use AI" is not a policy—it's a prohibition that invites workarounds.

[INPUT NEEDED: What patterns have you observed at GGCS? What do your teachers report about WHY students reach for AI? Any specific examples or conversations that illuminate this?]

**The schools seeing results have shifted from detection to design.** Research shows that schools implementing assessment redesign see 40% fewer AI-related integrity issues compared to detection-only approaches.

What does assessment redesign actually mean?

It means assignments where AI assistance is visible, not hidden. Students write first, then use AI to critique their work—and explain what feedback they accepted or rejected. The thinking becomes the product, not just the output.

It means process over product. Require drafts, revision histories, or in-class components that make the learning journey visible. When you can see how a student arrived at their final work, the question of AI use becomes less fraught.

It means authentic tasks that resist automation. Assignments connected to local context, personal experience, or original research can't be completed by prompting ChatGPT. A student analyzing their own community, interviewing a family member, or building on their previous work creates something AI can't replicate.

It means making AI use explicit and bounded. Instead of "don't use AI," specify: "You may use AI for brainstorming but not for drafting. You may use AI to check grammar but must document any suggestions you accepted." Clear boundaries are easier to follow than vague prohibitions.

<BeforeAfter 
  before="Write a 5-paragraph essay on the themes of The Great Gatsby"
  after="Interview a family member about their version of 'the American Dream.' Compare their perspective to Gatsby's. Use AI to help you identify themes in the interview transcript, but write the analysis yourself."
/>

**What this looks like in practice.** MagicSchool's CEO Adeel Khan describes the shift: "In 2023 the fear was simple: 'Kids will use AI to cheat.' By the end of 2026, the bigger surprise will be how many students use AI to do more thinking, not less, in schools that teach them how."

Students draft on their own, then use AI for formative feedback aligned to the teacher's rubric. They ask "Why is this a weak thesis?" or "How could I make this clearer?" instead of "Write this for me." They compare AI suggestions to the rubric and explain how they used AI as part of the assignment, instead of hiding it.

The technology didn't change. The adult framing did.

[INPUT NEEDED: Have you or your teachers tried any redesigned assessments? What worked? What didn't? Even a small example would ground this section.]

**Where to start.** Pick one assignment this quarter—preferably one you suspect students are already using AI on—and redesign it using these principles:

Add a process component. Require a brainstorming document, a first draft, or revision notes that show the work behind the work.

Make AI use explicit. Specify what's allowed, what's not, and what documentation is required. "You may use AI for X but not Y. If you use AI for X, note it in your submission."

Connect to something AI can't access. Local context, personal experience, original data, or in-class discussion that happened after the AI's training cutoff.

The goal isn't to make cheating impossible. The goal is to make learning visible—and to make authentic engagement more rewarding than shortcuts.

[INPUT NEEDED: What's one assignment at GGCS that would be a good candidate for this kind of redesign? Walking through a specific example would make this actionable.]

---

### Your Input Needed for Post 1

1. **Opening story:** A specific moment when detection failed or felt futile at GGCS or in your network
2. **Student motivation patterns:** What have you observed about WHY students reach for AI? Specific examples?
3. **Assessment redesign attempts:** Have you or your teachers tried any? Results?
4. **Specific example:** An assignment that could be redesigned—ideally one you've actually modified or plan to

---

# POST 2: Teacher Training Gap

## Title: "68% of Teachers Have Zero AI Training. What That Means for Your School."

### Outline

**Hook:** The statistic that should alarm every school leader

**The Meat:**
- Section 1: What "no training" actually looks like in classrooms
- Section 2: Why traditional PD approaches fail for AI
- Section 3: What effective AI training looks like
- Section 4: The time savings that justify the investment

**Bottom Line:** A realistic path to building teacher AI capacity

---

### Draft

Here's a number that should concern every school leader: 68% of urban teachers report receiving no AI training since joining their schools.

Not inadequate training. Not outdated training. None.

Meanwhile, 92% of students are using AI tools. 83% of K-12 teachers are using generative AI for personal or school-related activities. The tools are everywhere. The guidance is nowhere.

[INPUT NEEDED: What was the training situation at GGCS when you arrived? What have you done since? This grounds the piece in your actual experience.]

**What "no training" looks like in practice.** Teachers are making it up as they go. Some ban AI entirely—then struggle to enforce it. Some ignore it—then feel blindsided when problems emerge. Some embrace it enthusiastically—then realize they've inadvertently violated data privacy policies or created academic integrity nightmares.

The 2025-2026 Education Insights Report makes this clear: while district leaders see AI as an opportunity, classroom teachers—who manage distraction, plagiarism, and unclear policies every day—approach it with more caution. There's a gap between leadership optimism and teacher reality.

That gap creates problems. Teachers who don't feel confident with AI can't teach students to use it responsibly. They can't make informed decisions about which tools to adopt. They can't have substantive conversations with parents who have questions. They're stuck between a mandate to innovate and a lack of support for doing so.

<Callout type="warning" title="The confidence gap">
70% of teachers and administrators admit they don't feel ready to use AI effectively in their classrooms. Yet they're expected to guide students through a technology transformation. The math doesn't work.
</Callout>

**Why traditional PD approaches fail for AI.** Most professional development treats AI like any other tool—a one-day workshop, a slide deck, a list of approved applications. This doesn't work for three reasons.

AI changes faster than annual training cycles. The tool you demo in August may be obsolete by October. Training on specific applications becomes dated quickly. Teachers need frameworks for evaluating new tools, not just familiarity with current ones.

AI requires hands-on practice, not just exposure. You can't understand what AI can and can't do by watching a presentation. You have to use it—experiment, fail, iterate. Most PD doesn't build in this kind of exploratory time.

AI decisions are contextual, not universal. Whether to allow AI on a specific assignment depends on learning objectives, student needs, and assessment design. Generic policies don't help teachers make these judgment calls. They need practice reasoning through specific scenarios.

[INPUT NEEDED: What PD approaches have you tried at GGCS? What worked? What felt like a waste of time? Your experience training 50+ teachers is directly relevant here.]

**What effective AI training actually looks like.** The research points to several principles that separate useful training from checkbox exercises.

Ongoing over one-shot. Instead of annual workshops, create professional learning communities where teachers meet regularly to share experiments, analyze results, and problem-solve together. A grade-level team examining how AI affected a particular assignment learns more than a whole-staff session on "AI basics."

Practice-embedded over theory-heavy. Teachers need to use AI for their own work—lesson planning, communication, administrative tasks—before they can guide students. Start with applications that save teachers time. Build competence through experience.

Scenario-based over policy-based. Rather than memorizing rules, teachers benefit from working through realistic situations. "A student submits work that reads like AI but claims it's original. What do you do?" Practice with gray areas builds judgment that policies alone can't provide.

Peer-led over expert-delivered. Teachers learn well from colleagues who face the same constraints and contexts. Identify early adopters in your building and create structures for them to share what's working. External experts have their place, but internal expertise scales better.

<Callout type="tip" title="The time investment that pays off">
Teachers using AI tools at least weekly save an average of 5.9 hours per week—roughly 6 extra weeks of reclaimed time across a school year. Training that helps teachers capture even half those savings pays for itself many times over.
</Callout>

**The time savings that justify the investment.** Here's the thing about AI training: it's an investment that generates immediate returns for teachers who are already stretched thin.

HMH's annual educator survey found that 68% of teachers using AI report saving up to five hours per week. That's time reclaimed for instruction, relationship-building, or—let's be honest—not burning out.

The most impactful applications aren't the flashy ones. They're the mundane time-sinks that AI handles well: drafting parent communications, creating differentiated materials, summarizing meeting notes, generating quiz questions. When teachers see AI save them real time on real tasks, adoption follows naturally.

[INPUT NEEDED: What time savings have you personally experienced? What about your teachers? Specific examples make this concrete.]

**A realistic path forward.** You don't need a comprehensive AI curriculum to start. You need momentum.

**This month:** Survey your teachers about their current AI use—both personal and professional. You'll likely find more experimentation happening than you knew. Identify your early adopters.

**This quarter:** Create a low-stakes space for sharing. A monthly lunch session, a shared document, a Slack channel—whatever fits your culture. Let teachers who are experimenting share what they're learning. Make it safe to discuss failures.

**This semester:** Move from informal sharing to structured practice. Pick one high-value use case—parent communication, lesson planning, or feedback on student work—and give teachers time to experiment with it. Provide specific prompts and examples, not just permission.

**Ongoing:** Build AI into existing PD structures rather than treating it as a separate initiative. When you discuss assessment design, include AI considerations. When you review curriculum, examine AI implications. Normalize AI as part of how you talk about teaching.

The goal isn't to make every teacher an AI expert. It's to build enough fluency that teachers can make informed decisions, have productive conversations, and model thoughtful AI use for students.

[INPUT NEEDED: What's your actual plan for AI training at GGCS? What would you recommend to other school leaders based on what you've tried?]

---

### Your Input Needed for Post 2

1. **Training situation:** What was the state of AI training at GGCS when you started? What have you implemented?
2. **PD approaches:** What have you tried? What worked vs. felt like a waste of time?
3. **Time savings:** Specific examples from your experience or your teachers' experience
4. **Your recommendations:** What would you tell another school leader about building teacher capacity?

---

# POST 3: Deepfakes & Student Safety

## Title: "The Deepfake Crisis Schools Aren't Ready For"

### Outline

**Hook:** The scale of the problem—from 4,700 reports to 440,000 in 18 months

**The Meat:**
- Section 1: What's actually happening in schools
- Section 2: Why schools are unprepared
- Section 3: The legal landscape (TAKE IT DOWN Act, state laws)
- Section 4: What schools need to do now

**Bottom Line:** Concrete steps for prevention and response

---

### Draft

In 2023, the National Center for Missing and Exploited Children received 4,700 reports of AI-generated child sexual abuse images.

In just the first six months of 2025, that number hit 440,000.

That's not a typo. That's a 93x increase in eighteen months.

And a significant portion of these images are being created by students, of students, in our schools.

[INPUT NEEDED: Has deepfake content been an issue at GGCS or in your professional network? Even if you haven't dealt with it directly, your awareness of the issue matters here.]

**What's actually happening.** The cases coming to light follow a disturbing pattern. In Iowa, four boys were charged for creating deepfake nude images of 44 female classmates using photos from social media. In Louisiana, a middle school student was expelled—not for creating deepfakes, but for fighting with classmates who were sharing AI-generated nude images of her. In New Jersey, a federal lawsuit was filed against ClothOff, an AI tool used by a minor to create explicit images of a 15-year-old classmate.

These aren't isolated incidents. A RAND survey found that 13% of K-12 principals reported deepfake bullying incidents during 2024-25. The Center for Democracy and Technology reports that 12% of students know of nonconsensual intimate AI-generated imagery depicting someone in their school community. Among students at schools that have heavily adopted AI, 21% are aware of such imagery.

The technology has become trivially easy to use. "Now, you can do it on an app, you can download it on social media, and you don't have to have any technical expertise whatsoever," says Sergio Alexander of Texas Christian University. What once required technical skill now requires only a smartphone and a willingness to harm.

<Callout type="warning" title="The victims are almost always girls">
Between 40-50% of students are aware of deepfakes circulating at school. The victims are overwhelmingly female. The perpetrators are often their male classmates. The emotional and psychological impact is severe and long-lasting.
</Callout>

**Why schools are unprepared.** Most schools have no policy specifically addressing AI-generated imagery. Over half of educators report receiving no training—or poor-quality training—on deepfakes. When incidents occur, administrators are often "more confused than we were," as one victim described it.

The Iowa case illustrates the gap. When 44 girls discovered they'd been targeted, the school's initial response was to notify parents with a warning not to talk about it. No one checked on the victims' wellbeing the next day. No counseling was offered. The girls eventually issued a joint public statement—"Voices of the Strong 44"—demanding better support and policy changes.

Schools are caught between competing pressures. The technology is moving faster than policy. Legal frameworks are still emerging. Administrators aren't sure what they're allowed to do. The result is reactive scrambling rather than proactive protection.

**The legal landscape is shifting.** The TAKE IT DOWN Act, signed into federal law in 2025, makes it a crime to publish nude or explicit images—whether real or fake—of a child. Online platforms must remove nonconsensual images within 48 hours of a victim's report. Schools have specific obligations around reporting and response.

At the state level, at least half of states enacted deepfake legislation in 2025. Iowa updated its law to make creating AI-generated sexual content of minors a crime. Louisiana prosecuted the first case under its new law. Massachusetts requires violators to complete an education program about laws and impacts.

But laws alone aren't enough. Students creating these images often don't realize they're committing crimes. They may see it as a prank or a joke, not understanding the consequences for victims or for themselves. Criminal penalties after the fact don't prevent harm—and incarcerating youth offenders doesn't solve the underlying problem.

[INPUT NEEDED: What's the legal/policy situation in Indonesia where GGCS is located? How does this translate for international schools? Any guidance from IB or other bodies?]

**What schools need to do now.** Experts recommend a multi-pronged approach combining policy, education, and response protocols.

**Update your acceptable use policies.** Make explicit that creating, sharing, or possessing AI-generated intimate imagery of any person without consent is prohibited. Specify consequences. Ensure the policy covers off-campus behavior that affects the school community.

**Train your staff.** Administrators need to understand what deepfakes are, how they're created, and how to respond when incidents occur. Teachers need to recognize warning signs. Counselors need protocols for supporting victims. A 17-minute training course created by a teenage survivor and cybersecurity firms is now available free to schools.

**Educate students proactively.** Digital citizenship curricula should address AI-generated content specifically. Students need to understand what crosses the line into illegal behavior. They need to know what to do if they discover deepfakes of themselves or friends. Use the SHIELD framework: Stop (don't forward), Huddle (with a trusted adult), Inform (platforms), Evidence (document who's spreading it without downloading), Limit (social media access), Direct (victims to help).

**Create clear reporting and response protocols.** Establish efficient processes to receive, investigate, and act on reports. Under the TAKE IT DOWN Act, schools must take steps to remove or restrict access to reported content within 48 hours. Coordinate with platforms and law enforcement. Document everything.

**Support victims immediately.** When incidents occur, prioritize the wellbeing of targeted students. Offer counseling. Check in regularly. Don't silence victims or make them feel responsible for the situation. The Iowa students' demand was simple: they wanted someone to check on them the next day. That shouldn't be too much to ask.

<Callout type="tip" title="Resources for schools">
The National Center for Missing and Exploited Children offers resources including the CyberTipline, guidance for removing images, and emotional support. Adaptive Security and Pathos Consulting Group offer free training courses. The Future of Privacy Forum has guidance on vetting AI tools.
</Callout>

**The conversation we need to have.** Laura Tierney of The Social Institute emphasizes that education beats reaction. But that education needs to happen before incidents occur—not after.

Talk about consent, autonomy, and respect for others. Talk about the permanence of digital content and the real harm that "just a joke" can cause. Talk about what to do when you encounter harmful content, and make clear that students can come to adults without getting in trouble.

Mary Anne Franks of the Cyber Civil Rights Initiative puts it simply: "We should just stick to the things that we know, which don't change with technology, which is consent, autonomy, agency, safety. Those are all things that should be at the heart of what we talk to kids about."

The technology will keep evolving. The values conversation doesn't have to wait.

[INPUT NEEDED: How do you approach digital citizenship and online safety at GGCS? What conversations have you had with students, teachers, or parents about this? Your perspective as a school leader on these difficult topics would strengthen the conclusion.]

---

### Your Input Needed for Post 3

1. **Direct experience:** Any encounters with deepfake issues at GGCS or in your network?
2. **International context:** How does this translate for international schools? Indonesia-specific considerations?
3. **Digital citizenship approach:** How does GGCS handle these conversations with students?
4. **Parent communication:** How would/do you communicate about this issue with parents?

---

# POST 4: AI Policy Framework

## Title: "Your School Needs an AI Policy. Here's a Framework That Actually Works."

### Outline

**Hook:** The policy vacuum most schools are operating in

**The Meat:**
- Section 1: Why generic policies fail
- Section 2: The three domains your policy must address
- Section 3: A framework for decision-making (not just rules)
- Section 4: How to build buy-in across stakeholders

**Bottom Line:** A template approach you can adapt

---

### Draft

Only 10% of schools and universities have formal AI use guidelines.

That means 90% of educators are making judgment calls without a shared framework. Teachers in the same building apply different standards. Students get different messages from different classes. Parents have no idea what the rules are—because the rules don't exist.

This isn't sustainable.

[INPUT NEEDED: What was the policy situation at GGCS when you arrived? Did you have to create AI guidelines from scratch? What prompted you to act?]

**Why generic policies fail.** Many schools respond to the AI challenge by adopting broad prohibitions: "AI is not permitted on assignments." This approach fails for three reasons.

It's unenforceable. As we've established, AI detection is unreliable. Students who want to use AI can do so without getting caught. A policy you can't enforce undermines the credibility of all your policies.

It's too blunt. AI use exists on a spectrum from clearly problematic (having AI write your essay) to clearly beneficial (using AI to check grammar or generate brainstorming prompts). A blanket ban treats all uses as equivalent, which they're not.

It ignores the world students are entering. Students will use AI in college and in their careers. A school that bans AI entirely isn't preparing students for that reality—it's hiding from it.

The other extreme—no policy at all—creates different problems. Teachers feel unsupported. Students feel confused. Parents feel anxious. When something goes wrong, there's no framework for addressing it.

<Callout type="story" title="A tale of two classrooms">
A student uses AI to brainstorm ideas for an essay. In one classroom, this is encouraged. In the next classroom, it's a violation. Same student, same tool, different consequences. That inconsistency isn't fair to students—and it reflects a policy vacuum that leadership hasn't addressed.
</Callout>

[INPUT NEEDED: Can you share a specific example of inconsistency you've observed or had to address? This grounds the problem in reality.]

**The three domains your policy must address.** Effective AI policy covers three distinct areas, each with different considerations.

**Domain 1: Student use of AI.** This is where most schools focus—and where the most variation exists. Key questions include:
- What AI use is permitted on assessments? Under what conditions?
- How should students document or disclose AI assistance?
- What consequences follow from undisclosed use?
- How does this vary by grade level, subject, or assignment type?

**Domain 2: Teacher use of AI.** Often overlooked, but equally important. Questions include:
- Can teachers use AI for lesson planning, communication, or feedback?
- What data privacy considerations apply when teachers use AI tools?
- What transparency is required when AI assists in creating materials?
- How do we ensure AI-assisted feedback maintains human judgment?

**Domain 3: Institutional use of AI.** The school itself uses AI in various ways. Questions include:
- What AI tools are approved for school use?
- How do we vet new tools for privacy, security, and effectiveness?
- What data can be shared with AI systems?
- Who makes decisions about AI adoption?

Most schools address only the first domain—and even then, inconsistently. A comprehensive policy covers all three.

**A framework for decision-making.** Rather than trying to write rules for every scenario, effective policies provide frameworks that help people make good decisions.

One useful framework: the **AI Transparency Spectrum**. For any use of AI, consider:

| Level | Description | Example |
|-------|-------------|---------|
| **Prohibited** | AI cannot be used for this task | Final exams, original creative work where voice is essential |
| **Disclosed** | AI may be used but must be documented | Research assistance, editing, brainstorming |
| **Encouraged** | AI use is expected or beneficial | Checking grammar, generating practice problems |
| **Required** | Learning to use AI is part of the objective | AI literacy assignments, prompt engineering practice |

Different assignments fall at different points on the spectrum. The policy doesn't need to specify where every assignment falls—it needs to establish that teachers communicate expectations clearly and that students document use when required.

Another useful framework: the **Purpose Test**. Before using AI, ask:
- What is the learning objective of this task?
- Does AI assistance help or hinder that objective?
- Is the AI doing the thinking, or helping the student think?

If an assignment is meant to develop writing skills, having AI write the essay defeats the purpose. If an assignment is meant to develop analytical skills, using AI to generate the initial draft might free students to focus on analysis.

[INPUT NEEDED: What frameworks have you developed or adopted at GGCS? How do you communicate expectations to teachers and students? Any specific language or approaches you've found effective?]

**How to build buy-in.** A policy imposed from the top will generate resistance. A policy developed collaboratively builds ownership. Here's a process that works:

**Start with listening.** Survey teachers about their current AI use and concerns. Survey students about their use and questions. Survey parents about their expectations. You'll learn things that surprise you—and you'll identify stakeholders who should be part of the development process.

**Form a representative working group.** Include teachers from different subjects and grade levels, at least one administrator, and ideally a student representative. A diverse group surfaces considerations you'd miss and creates ambassadors who can explain the policy to their peers.

**Draft, test, revise.** Create a draft policy. Test it against realistic scenarios. "A student uses AI to translate an essay from their native language—is this permitted?" "A teacher uses AI to generate report card comments—is this appropriate?" Scenarios reveal gaps and ambiguities.

**Communicate clearly and repeatedly.** Once adopted, the policy needs to reach everyone—teachers, students, parents. Don't just post it on the website. Discuss it in staff meetings. Address it in class. Send a parent communication. Explain the reasoning, not just the rules.

**Plan for iteration.** AI is changing fast. Your policy will need updates. Build in a review cycle—perhaps quarterly in the first year, then annually. Make clear that the policy is a living document, not a permanent decree.

<BeforeAfter 
  before="AI is prohibited on all assignments unless otherwise specified"
  after="AI use expectations are communicated for each assignment using our transparency spectrum. When in doubt, ask. When you use AI, document how."
/>

**A starting template.** Here's a simplified framework you can adapt:

**Purpose:** This policy guides the responsible use of AI tools by students, teachers, and staff, supporting learning while maintaining academic integrity.

**Student Use:**
- AI use expectations are specified for each assignment by the teacher
- When AI use is permitted, students document what tools they used and how
- Undisclosed AI use where disclosure was required is an academic integrity violation
- Students are expected to develop AI literacy as part of their education

**Teacher Use:**
- Teachers may use AI for planning, communication, and feedback
- AI-generated content should be reviewed and edited before use
- Student data should not be entered into AI tools without appropriate safeguards
- Teachers communicate clear AI expectations for each assessment

**Institutional Use:**
- AI tools must be vetted for privacy and security before adoption
- [Designated role] approves new AI tools for school use
- Data shared with AI systems must comply with FERPA/COPPA/local requirements
- AI adoption decisions consider equity, access, and pedagogical value

**Review:** This policy will be reviewed [quarterly/annually] and updated as needed.

[INPUT NEEDED: What does your actual GGCS AI policy look like? Can you share language you've developed? Even a partial example would be valuable.]

---

### Your Input Needed for Post 4

1. **Policy creation story:** What prompted you to develop AI guidelines at GGCS? What process did you follow?
2. **Specific frameworks:** What language or approaches have you found effective?
3. **Building buy-in:** How did you get teachers and parents on board?
4. **Your policy (if shareable):** Even excerpts would ground this piece in real practice

---

# POST 5: Assessment Redesign

## Title: "How to Make Assignments AI-Proof (Without Banning AI)"

### Outline

**Hook:** The futility of the ban vs. the power of design

**The Meat:**
- Section 1: What makes assignments vulnerable to AI
- Section 2: Design principles that resist AI shortcuts
- Section 3: Examples across subjects and grade levels
- Section 4: The bonus: these assignments are better anyway

**Bottom Line:** A practical toolkit for redesigning common assignments

---

### Draft

Banning AI on assignments is like banning calculators in a math class where everyone has a phone. You can try, but you're fighting the tools students carry in their pockets.

The alternative isn't to give up on academic integrity. It's to design assignments where AI assistance is visible, bounded, or irrelevant—where authentic engagement becomes more rewarding than shortcuts.

Here's the good news: assignments designed to resist AI misuse tend to be better assignments anyway.

[INPUT NEEDED: Have you or your teachers experimented with AI-resistant assignment design? Even one example would anchor this piece.]

**What makes assignments vulnerable.** Some assignments practically invite AI completion. Understanding why helps you redesign them.

**Generic prompts with generic answers.** "Write an essay on the themes of The Great Gatsby" can be answered by any AI in seconds. The prompt doesn't require anything specific to the student—their perspective, their experience, their unique context.

**Product-only assessment.** When you only see the final output, you can't distinguish AI-generated work from student-generated work. The process is invisible.

**Disconnected from instruction.** If the assignment doesn't build on class discussion, in-class activities, or prior student work, there's nothing anchoring it to the actual learning that happened.

**Time-shifted without support.** Take-home assignments with no process components put the work entirely outside your view. Students facing time pressure make expedient choices.

<Callout type="warning" title="The vulnerability test">
Ask yourself: Could a student complete this assignment in full by copying my prompt into ChatGPT? If yes, the assignment is vulnerable. If no—because it requires their experience, our class discussion, or iterative revision—it's more resistant.
</Callout>

**Design principles that resist AI shortcuts.** You don't need to reinvent every assignment. Small modifications can make a big difference.

**Principle 1: Require process, not just product.** Add components that make the learning journey visible. Brainstorming notes, annotated drafts, revision reflections, or recorded explanations show how a student arrived at their final work. AI can generate a polished essay; it can't generate a believable process that matches a specific student's development.

**Principle 2: Connect to local, recent, or personal context.** AI knows nothing about what happened in your class yesterday, your school community, or a student's personal experience. Assignments that build on these contexts require authentic engagement.

**Principle 3: Include in-class components.** A hybrid structure where some work happens in class and some at home gives you visibility into the student's capabilities. An outline developed in class, a draft reviewed in conference, or an in-class presentation of take-home work creates checkpoints.

**Principle 4: Make AI use explicit and bounded.** Instead of prohibiting AI, specify how it can be used. "Use AI to generate counterarguments to your thesis, then address them in your essay." The AI assistance is visible, documented, and part of the learning.

**Principle 5: Design for iteration.** Assignments with multiple drafts and revision cycles are harder to complete with a single AI prompt. The iterative process—and teacher feedback along the way—creates a paper trail that reveals authentic engagement.

[INPUT NEEDED: Which of these principles have you found most practical to implement? Are there others you'd add?]

**Examples across subjects.** Here's how these principles apply in practice:

**English/Language Arts:**
- Before: Write an essay analyzing symbolism in a novel
- After: Select a symbol from the novel that connects to something in your own life. In your first draft, explore that personal connection. Use AI to identify weaknesses in your argument, then revise. Submit both drafts with a reflection on how you used AI feedback.

**Science:**
- Before: Write a lab report on the experiment
- After: Explain your lab results to a family member with no science background. Record their questions. Write a reflection on what their questions revealed about gaps in your understanding. Use AI to check your technical accuracy, but the explanation must be yours.

**Social Studies:**
- Before: Research and present on a historical event
- After: Interview someone who lived through [relevant era/event] or research your local community's connection to the historical event. Your presentation must include at least three primary sources you gathered yourself.

**Math:**
- Before: Solve these problems for homework
- After: Solve problems 1-5. For problem 6, use AI to generate a solution, then find and explain any errors. For problem 7, create an original problem in the same format and explain your thinking.

**World Languages:**
- Before: Write a paragraph in [target language] about your weekend
- After: Draft your paragraph. Use AI to check grammar. Highlight every correction the AI made. In class, explain three corrections and what you learned from them.

<BeforeAfter 
  before="Research a social issue and write a persuasive essay"
  after="Interview three people in our community about their views on [local issue]. Analyze the different perspectives and develop your own position. Your essay must directly reference your interviews."
/>

**The bonus: these assignments are better anyway.** Here's what I've noticed: assignments designed to resist AI shortcuts tend to produce better learning outcomes even when AI isn't a concern.

Requiring process makes thinking visible—which helps teachers identify where students are struggling. Connecting to local context increases engagement—students care more about their community than abstract topics. Including in-class components builds relationships—you learn more about your students. Making AI use explicit teaches metacognition—students learn to think about how they think.

You're not just making assignments AI-proof. You're making them better.

[INPUT NEEDED: Have you observed this "bonus effect"? Specific examples of redesigned assignments producing better engagement or learning?]

**A practical toolkit.** For any existing assignment, run through this checklist:

□ Does the assignment require something specific to this student (their experience, perspective, prior work)?
□ Does the assignment connect to something AI can't access (class discussion, local context, recent events)?
□ Does the assignment make the process visible (drafts, notes, reflections)?
□ Does the assignment include any in-class component (brainstorming, conferencing, presentation)?
□ Does the assignment specify how AI can and cannot be used?

If you answered "no" to all five, the assignment is highly vulnerable. Aim to answer "yes" to at least two or three.

You don't need to redesign everything at once. Start with one assignment that you suspect students are already using AI on. Apply two principles. See what happens. Iterate.

[INPUT NEEDED: What's one assignment you or a colleague has successfully redesigned? Walk through the before/after?]

---

### Your Input Needed for Post 5

1. **Redesign examples:** Specific assignments you or your teachers have modified
2. **Principle preferences:** Which design principles have you found most practical?
3. **"Bonus effect" evidence:** Have redesigned assignments produced better engagement?
4. **Subject-specific ideas:** Particularly for IB contexts (PYP/MYP) if relevant

---

# POST 6: Parent Communication

## Title: "What Parents Are Asking About AI—And How to Answer"

### Outline

**Hook:** The questions hitting your inbox (and the anxiety behind them)

**The Meat:**
- Section 1: The five questions parents actually ask
- Section 2: The fears behind the questions
- Section 3: How to respond (with sample language)
- Section 4: Proactive communication that prevents anxiety

**Bottom Line:** A communication framework for this ongoing conversation

---

### Draft

Parent emails about AI started trickling in last year. Now they're a steady stream.

"Is my child allowed to use ChatGPT for homework?"
"How do you know if students are cheating with AI?"
"Are you teaching my child to use AI, or to avoid it?"
"Should I be worried about AI at school?"

Behind every question is anxiety—and often, confusion. Parents are getting mixed messages from media, from other parents, from their own children. They need clarity, and they're looking to school leaders to provide it.

[INPUT NEEDED: What AI questions are you actually getting from GGCS parents? Any specific emails or conversations you can reference (anonymized)?]

**The five questions parents actually ask.** In my experience, parent questions about AI cluster around five themes:

**Question 1: What are the rules?** Parents want to know what's allowed and what isn't. They need clarity so they can support expectations at home. "My daughter says she can use AI for research but not writing—is that right?"

**Question 2: How do you catch cheating?** Parents assume detection is both possible and happening. They want assurance that academic integrity is being maintained. "How do you know if a student used AI on their essay?"

**Question 3: Is my child learning or taking shortcuts?** This is the deeper fear. Parents worry that AI is enabling laziness, that their children are missing fundamental learning. "If my son uses AI to help with math, is he actually learning math?"

**Question 4: What are you teaching about AI?** Some parents want the school to teach AI skills. Others want the school to teach AI avoidance. Both are asking the same question: what's the school's stance on preparing kids for an AI world?

**Question 5: Should I be worried?** This is the meta-question beneath the specific ones. Parents want reassurance—or honest acknowledgment—about whether AI is a threat to their child's education.

<Callout type="story" title="The question behind the question">
When a parent asks "How do you detect AI cheating?", they're rarely asking for a technical explanation of detection tools. They're asking: "Can I trust that my child's grades mean something? Are you maintaining standards?" Answer the question behind the question.
</Callout>

**The fears behind the questions.** Understanding parent anxiety helps you respond more effectively.

**Fear of unfairness.** Parents worry that some students are using AI while others aren't—and that honest students are being disadvantaged. They want a level playing field.

**Fear of skill gaps.** Parents worry their children are missing fundamental learning by outsourcing thinking to AI. If kids can't write without AI now, what happens in college? In careers?

**Fear of being out of the loop.** Many parents don't understand AI themselves. They feel unable to guide their children or even know if there's a problem. The technology moved faster than their knowledge.

**Fear of schools being behind.** Parents sense that technology is changing faster than institutions can adapt. They worry schools are either too permissive or too restrictive—and either way, not preparing students for reality.

[INPUT NEEDED: What fears do you hear from GGCS parents? Are there concerns specific to your international school context?]

**How to respond.** Here's sample language for each common question:

**"What are the rules about AI?"**
"Our policy is that AI expectations are set for each assignment by the teacher. Some assignments allow AI assistance with documentation. Others don't allow AI at all. When in doubt, students should ask before using AI, and always document when they do. We communicate these expectations clearly, and we expect students to be honest about their use. We're happy to share our full AI policy if that would be helpful."

**"How do you detect AI use?"**
"We focus less on detection and more on design. Detection tools are unreliable and create an adversarial dynamic we want to avoid. Instead, we design assignments where the learning process is visible—through drafts, in-class components, and documentation of how students worked. This approach maintains integrity while preserving trust between teachers and students."

**"Is my child actually learning?"**
"This is exactly the question we're grappling with as educators. Our approach is to think carefully about which tasks develop skills students need and which tasks AI can reasonably assist with. When writing skill development is the goal, we design assignments that require original thinking and visible process. When research skills are the goal, we might allow AI assistance for gathering information while requiring students to analyze and synthesize themselves. We're trying to prepare students to use AI effectively while ensuring they develop fundamental capabilities."

**"What are you teaching about AI?"**
"We're teaching students to use AI as a tool, not a crutch. That means understanding what AI can and can't do well, knowing when AI assistance is appropriate versus when it shortcuts learning, and developing judgment about how to use technology responsibly. We believe students will use AI throughout their lives, so they need to learn to use it thoughtfully—neither avoiding it entirely nor depending on it uncritically."

**"Should I be worried?"**
"We take this seriously and we're being thoughtful about it. AI presents both opportunities and challenges for education. We're working to capture the benefits—efficiency, personalization, new forms of learning—while protecting against the risks—shortcuts, integrity issues, over-reliance. We don't have all the answers, and this will keep evolving. What we can promise is that we're engaged with these questions, we're updating our approaches as we learn, and we welcome your partnership in navigating this together."

[INPUT NEEDED: Have you developed specific language for parent communication? What's worked well?]

**Proactive communication that prevents anxiety.** The best parent communication happens before concerns become complaints. Consider:

**Early and clear policy communication.** Share your AI policy at the start of the year. Explain the reasoning, not just the rules. Invite questions.

**Regular updates.** As AI evolves, update parents on how your approach is evolving. A quarterly note in your newsletter keeps the conversation ongoing.

**Parent education opportunities.** Offer a session where parents can ask questions and learn about AI. Many parents are curious but don't know what to ask. Creating space for the conversation reduces anxiety.

**Modeling transparency.** When the school uses AI—for communication, for planning, for administration—be transparent about it. Modeling responsible use reinforces expectations.

<Callout type="tip" title="A useful frame">
Frame AI as similar to earlier technology shifts—calculators, internet research, word processors. Each raised concerns about shortcuts and skill loss. Each required thoughtful integration rather than blanket bans or uncritical adoption. We've navigated these transitions before, and we can navigate this one.
</Callout>

[INPUT NEEDED: What proactive communication have you done with GGCS parents about AI? What would you recommend to other school leaders?]

---

### Your Input Needed for Post 6

1. **Actual parent questions:** What AI questions are GGCS parents asking?
2. **Parent fears:** What concerns seem most prominent in your community?
3. **Language you've developed:** Any specific responses that have worked well?
4. **Proactive communication:** What have you done to get ahead of concerns?

---

# POST 7: AI Tutoring Reality Check

## Title: "AI Tutoring: What the Research Actually Says"

### Outline

**Hook:** The hype vs. the evidence

**The Meat:**
- Section 1: What the Harvard study actually found (and its limitations)
- Section 2: What systematic reviews show about intelligent tutoring systems
- Section 3: The conditions under which AI tutoring works
- Section 4: Implications for school leaders evaluating AI tutoring tools

**Bottom Line:** Questions to ask before adopting AI tutoring

---

### Draft

The headline was irresistible: "AI Tutoring Outperforms Human Instruction." The June 2025 Harvard study made waves, showing students learned significantly more with an AI tutor than in active learning classrooms.

If you're a school leader evaluating AI tutoring tools, that headline probably landed in your inbox multiple times. Vendors cited it. Board members asked about it. Parents wondered if they should be hiring robot tutors.

But headlines rarely tell the whole story.

[INPUT NEEDED: Have you been pitched AI tutoring solutions at GGCS? How do you evaluate these claims?]

**What the Harvard study actually found.** The study was rigorous—a randomized controlled trial comparing college students learning physics from an AI tutor versus active learning instruction. The AI tutor produced significantly better learning outcomes with large effect sizes (0.73 to 1.3 standard deviations). Students felt more engaged and motivated.

But three things matter for school leaders thinking about implementation:

**The AI wasn't generic ChatGPT.** The researchers built a custom AI tutor designed around specific pedagogical best practices. It didn't just answer questions—it guided students through problem-solving, refused to give direct answers, managed cognitive load, and promoted growth mindset. This took significant design effort.

**The population was Harvard undergraduates.** The study raises questions about generalizability to community colleges, younger students, different subjects, or populations with varying technological comfort. The researchers acknowledge this limitation.

**The comparison was to active learning, not to human tutors.** Active learning classrooms are better than passive lectures, but they're not the same as one-on-one human tutoring. The study shows AI can outperform one teaching method, not that AI matches the full range of what human tutors provide.

<Callout type="warning" title="Read the fine print">
The study's authors explicitly note that "unguided use" of AI "lets students complete assignments without engaging in critical thinking" because "AI chatbots are generally designed to be helpful, not to promote learning." The success came from careful pedagogical design, not from deploying ChatGPT.
</Callout>

**What systematic reviews show.** While the Harvard study is promising, it's one study. Systematic reviews of intelligent tutoring systems across many studies paint a more nuanced picture.

A May 2025 review in *npj Science of Learning* analyzed 28 studies involving 4,597 K-12 students. The findings: effects on learning are "generally positive but are found to be mitigated when compared to non-intelligent tutoring systems."

In other words: AI tutoring helps, but it's not magic. And when compared to well-designed non-AI tutoring, the advantage shrinks.

The review also found that "ITS should be considered complementary tools rather than replacements for educators." The most effective implementations combine AI with teacher-led guidance. The technology works best as an enhancement to human instruction, not a substitute for it.

Another review noted that none of the studies examined considered AI ethics—a significant oversight given concerns about data privacy, bias, and appropriate use with minors.

[INPUT NEEDED: Have you used any AI tutoring tools at GGCS? What have you observed?]

**The conditions under which AI tutoring works.** Research points to several factors that separate effective AI tutoring from disappointing implementations:

**Pedagogically designed, not generically deployed.** AI tutors that incorporate learning science principles—active learning, appropriate challenge, immediate feedback, growth mindset messaging—outperform generic chatbots. The design matters more than the underlying AI model.

**Combined with human instruction.** AI tutoring works best as part of a blended approach where teachers remain involved. Studies on systems like WeWrite and Lynnette with Lumilo emphasize the complementary relationship. AI handles some tasks; humans handle others.

**Focused on specific, well-structured domains.** AI tutoring performs better in subjects with clear right and wrong answers (math, physics) than in subjects requiring nuanced judgment (ethics, literary interpretation). It excels at providing practice and feedback in structured domains.

**Accompanied by student training.** Students need to learn how to use AI tutors effectively—how to ask good questions, how to persist when stuck, how to verify AI feedback. Without this training, students may game the system or become passive.

**Integrated into the curriculum.** AI tutoring that aligns with what's happening in class produces better results than AI tutoring as a disconnected add-on. The technology should support learning objectives, not replace instructional design.

<BeforeAfter 
  before="Adopt AI tutoring platform → hope it improves outcomes"
  after="Define learning objectives → evaluate whether AI tutoring supports them → pilot with clear metrics → integrate with instruction → train students and teachers"
/>

**Questions to ask before adopting AI tutoring.** When evaluating AI tutoring tools, skeptical questions serve you well:

**What pedagogical principles inform the design?** Generic AI is not the same as pedagogically designed AI. Ask vendors to explain how their system promotes learning rather than just providing answers.

**What does the evidence actually show?** Ask for studies, but read them critically. Small sample sizes, short durations, and populations different from your students warrant caution. Effect sizes that shrink when compared to active controls suggest more modest benefits.

**How does this integrate with instruction?** AI tutoring as a supplement to teaching differs from AI tutoring as a replacement. Understand the intended use case and how it fits your instructional model.

**What data does the system collect?** AI tutoring systems often collect significant student data. Understand what's collected, how it's stored, and whether it complies with privacy requirements.

**What training is needed?** Both teachers and students need support to use AI tutoring effectively. Factor this into the implementation cost.

**What happens when students become dependent?** If students can only learn with AI support, what does that mean for their development? Consider the long-term implications, not just short-term results.

[INPUT NEEDED: What questions do you ask when evaluating EdTech tools? Any AI tutoring evaluations you've conducted?]

**The bottom line for school leaders.** AI tutoring can be valuable—but it's not a silver bullet. The research suggests:

Careful design matters more than AI capability. A well-designed AI tutor can outperform poorly designed human instruction. But a poorly designed AI tutor won't beat good teaching.

AI complements but doesn't replace teachers. The most effective implementations keep humans in the loop. Teacher judgment remains essential for motivation, relationship, and the aspects of learning that AI handles poorly.

Implementation quality determines outcomes. The same AI tool can produce great results or disappointing ones depending on how it's integrated, supported, and used.

Be appropriately skeptical of hype. Headlines about AI tutoring "outperforming" human instruction rarely reflect the nuanced reality. Ask hard questions. Demand evidence. Pilot before scaling.

[INPUT NEEDED: How do you approach EdTech adoption decisions at GGCS? What's your process for evaluating new tools?]

---

### Your Input Needed for Post 7

1. **AI tutoring pitches:** Have you been approached about AI tutoring solutions?
2. **Evaluation process:** How do you assess EdTech claims at GGCS?
3. **Direct experience:** Have you used any AI tutoring tools? What did you observe?
4. **Skeptical questions:** What questions do you ask vendors or consultants?

---

# POST 8: Student Data Privacy

## Title: "AI and Student Data: The Questions Every School Leader Should Ask"

### Outline

**Hook:** The privacy implications hiding in AI adoption

**The Meat:**
- Section 1: What data AI tools actually collect
- Section 2: FERPA, COPPA, and the 2025 amendments
- Section 3: Questions to ask before adopting AI tools
- Section 4: Building a privacy-respecting AI culture

**Bottom Line:** A practical vetting checklist

---

### Draft

When a teacher uses ChatGPT to draft a report card comment about a specific student, where does that data go?

When a student uses an AI tutor, what learning behaviors are tracked, stored, and potentially shared?

When your school adopts an AI-powered reading program, what happens to student performance data after the contract ends?

If you can't answer these questions, you have a data privacy problem—you just don't know it yet.

[INPUT NEEDED: Have you encountered data privacy concerns with AI tools at GGCS? Any specific situations that raised red flags?]

**What AI tools actually collect.** AI systems are data-hungry by design. The more data they have, the better they perform. This creates an inherent tension with privacy protection.

Generative AI tools like ChatGPT may retain conversation history for model training. If a teacher enters student names, grades, or behavioral information, that data could be absorbed into datasets without explicit consent.

AI tutoring systems track granular learning behaviors: time on task, error patterns, help-seeking behavior, attention patterns. This data can be valuable for personalization—but also sensitive if shared or misused.

AI detection tools like Turnitin process student work through external systems. Student essays may be stored, analyzed, and compared against future submissions.

AI writing assistants capture drafts, revisions, and feedback interactions. For students whose PII or work could be absorbed into datasets without consent, this raises authorship and ownership questions.

The principle of data minimization—collecting only what's necessary—often conflicts with AI's appetite for more data. Schools must navigate this tension consciously.

<Callout type="warning" title="The invisible data flow">
When you use a free AI tool, you're often paying with data. The terms of service that no one reads may grant broad permissions for data use, retention, and sharing. If students or teachers are using personal accounts with consumer AI tools, the school may have no visibility into what data is being shared.
</Callout>

**The legal landscape.** Federal and state regulations provide a framework, but application to AI remains evolving.

**FERPA** (Family Educational Rights and Privacy Act) protects educational records and limits disclosure of personally identifiable information. Schools using AI tools must ensure vendor agreements comply with FERPA and that parents can exercise their rights to access and correct records.

**COPPA** (Children's Online Privacy Protection Act) regulates data collection from children under 13. The 2025 amendments shifted the default from opt-out to opt-in consent and added new requirements for AI services.

**State laws** add additional requirements. Over 20 states reference FERPA, COPPA, and other regulations in their AI guidance. About 21 states list data security concerns as a focus, with varying requirements for encryption, authentication, and breach notification.

The challenge: AI tools often blur the line between school official (potentially allowed under FERPA) and third-party vendor (requiring stricter controls). Schools must make intentional decisions about how to classify AI services and what safeguards to require.

[INPUT NEEDED: How do data privacy regulations apply in Indonesia? Are there specific requirements for international schools or IB schools?]

**Questions to ask before adopting AI tools.** Before approving any AI tool for school use, work through these questions:

**Data collection:**
- What student data does this tool collect?
- Is every data point essential to the educational purpose?
- Could the same learning goals be achieved with de-identified data?

**Data storage:**
- Where is data stored? (Domestic servers? International jurisdictions?)
- How long is data retained?
- What happens to data when the contract ends?

**Data sharing:**
- Is data shared with third parties?
- Is data used for model training or improvement?
- Can families opt out of certain data uses?

**Security:**
- What security measures protect student data?
- Has the vendor experienced breaches? How did they respond?
- Is data encrypted in transit and at rest?

**Compliance:**
- Does the vendor provide FERPA compliance documentation?
- Does the vendor certify COPPA compliance for under-13 users?
- Are there independent audit reports or certifications?

**Vendor accountability:**
- What does the contract say about data ownership?
- What remedies exist if the vendor violates terms?
- How responsive is the vendor to compliance questions?

<Callout type="tip" title="Red flags">
Vendors who hesitate to provide compliance documentation are telling you something. "Trust us" is not a data protection policy. If a vendor can't clearly explain their data practices, consider that a warning sign.
</Callout>

**Building a privacy-respecting AI culture.** Beyond vetting tools, schools need cultures where privacy is a shared value.

**Create clear approval processes.** Designate who can approve new AI tools for school use. Create a vetting checklist that all tools must pass. Don't let individual teachers adopt tools without review.

**Train staff on data minimization.** Teachers should understand not to enter identifiable student information into AI tools without appropriate safeguards. Create guidance on what data is and isn't appropriate to share.

**Communicate with families.** Be transparent about what AI tools the school uses, what data they collect, and how privacy is protected. Create processes for families to ask questions or raise concerns.

**Document your decisions.** Keep records of what tools you've vetted, what questions you asked, and what you concluded. If problems arise, you want documentation showing due diligence.

**Review regularly.** AI tools change their terms of service. Vendors get acquired. Data practices evolve. Build in regular review cycles, not just one-time vetting.

[INPUT NEEDED: What vetting process do you use for EdTech tools at GGCS? How do you handle data privacy in your approval decisions?]

**A practical checklist.** For any AI tool under consideration:

□ Vendor provides clear documentation of data practices
□ Data collection is limited to what's necessary for educational purpose
□ Data storage location and retention policies are acceptable
□ Vendor does not use student data for model training without consent
□ Security measures meet school standards
□ Compliance certifications (FERPA, COPPA, or equivalent) are provided
□ Contract includes appropriate data ownership and breach notification terms
□ Families will be informed about the tool and data practices

If you can't check all boxes, the tool isn't ready for adoption—or requires additional safeguards.

[INPUT NEEDED: Do you have a checklist or vetting process you use? Sharing your actual approach would strengthen this section.]

---

### Your Input Needed for Post 8

1. **Privacy concerns encountered:** Any AI tools that raised data privacy red flags?
2. **Regulatory context:** How do data privacy requirements apply in Indonesia/international schools?
3. **Vetting process:** How do you evaluate EdTech tools for privacy at GGCS?
4. **Specific examples:** Any tools you've rejected or modified due to privacy concerns?

---

# POST 9: Critical Thinking Concerns

## Title: "Is AI Making Students Dumber? What the Research Shows"

### Outline

**Hook:** The fear every educator has (and whether it's justified)

**The Meat:**
- Section 1: What cognitive offloading actually means
- Section 2: What research shows about AI and learning
- Section 3: The cases where AI helps vs. hinders thinking
- Section 4: Designing for cognitive development, not just task completion

**Bottom Line:** Principles for using AI without losing learning

---

### Draft

Here's the fear that keeps educators up at night: What if we're creating a generation that can't think without AI?

It's not an unreasonable worry. Over 30% of students may become overly dependent on AI tools. 66% of workers admit to using AI outputs without verifying accuracy. Research warns of "learned passivity"—students skipping goal-setting, planning, and reflection because AI handles those steps.

The question isn't whether AI affects cognition. It's how—and what we can do about it.

[INPUT NEEDED: Have you observed students becoming dependent on AI? Specific examples from GGCS?]

**What cognitive offloading actually means.** Cognitive offloading is the process of using external tools to reduce mental effort. We've always done this—writing notes instead of memorizing, using calculators for arithmetic, saving phone numbers in contacts instead of learning them.

Some cognitive offloading is adaptive. No one argues we should memorize phone numbers. Some cognitive offloading is problematic. If you can't multiply single digits because you've always had a calculator, you've offloaded too much.

The question with AI is: which cognitive tasks should students develop internally, and which can be reasonably offloaded?

This isn't obvious, and reasonable people disagree. But ignoring the question—letting students offload whatever AI can handle—risks fundamental skill atrophy.

<Callout type="warning" title="The skill atrophy concern">
Research shows that traditional methods—discussion-based inquiry and teacher-student interaction—remain superior for fostering deep critical thinking and analytical skills. When students outsource thinking to AI, they may complete tasks faster while learning less.
</Callout>

**What research shows.** The evidence is mixed, which is the honest answer.

On the positive side: A 2025 Harvard study found that students using well-designed AI tutors learned more than twice as much in less time compared to traditional active-learning classrooms. When AI is designed to promote thinking rather than replace it, outcomes improve.

On the concerning side: Research warns of "learned passivity"—excessive dependence on AI can diminish genuine self-regulation, limiting learners' ability to direct their own study. When AI does the planning, monitoring, and reflecting, students may lose the ability to do these things independently.

The key variable seems to be whether AI is designed to guide thinking or shortcut it. Generic AI chatbots are designed to be helpful—which means giving answers. Pedagogically designed AI is designed to promote learning—which means guiding students toward answers without providing them directly.

Most AI tools students actually use are in the first category. The research showing benefits often comes from carefully designed systems in the second category.

[INPUT NEEDED: What have you observed about how GGCS students use AI? Are they using it to think, or to avoid thinking?]

**The cases where AI helps vs. hinders.** Not all cognitive tasks are equal. Understanding the differences helps you design appropriately.

**AI helps when:**
- The task involves information retrieval, not skill development (looking up facts)
- The student already has foundational skills and is working at a higher level (editing polished prose)
- The AI provides feedback that guides thinking rather than replacing it (explaining why an answer is wrong)
- The student maintains agency over decisions (using AI input as one consideration among many)

**AI hinders when:**
- The task is meant to develop a skill the student needs (practicing writing)
- The student doesn't have foundational skills to evaluate AI output (accepting errors)
- The AI provides answers without requiring thinking (skipping problem-solving)
- The student becomes passive, accepting AI decisions without reflection (learned helplessness)

The same tool can help or hinder depending on how it's used. A student using AI to check grammar after writing is different from a student having AI write the draft.

<BeforeAfter 
  before="Students use AI whenever it helps them complete tasks faster"
  after="Students use AI strategically, understanding which tasks develop skills they need and which tasks can be reasonably assisted"
/>

**Designing for cognitive development.** If your goal is task completion, let students use AI for everything. If your goal is learning, be more intentional.

**Identify the cognitive skills you're trying to develop.** For each assignment, be explicit: what thinking should students do? Writing skills? Analytical reasoning? Problem-solving? Research synthesis? Then protect those processes from AI shortcuts.

**Require thinking before AI assistance.** Students should draft before using AI to edit. They should attempt problems before using AI to check. The sequence matters: thinking first, AI second. When the sequence reverses, learning diminishes.

**Make AI assistance visible.** When students document how they used AI, they develop metacognitive awareness. "I used AI for X but not Y because..." requires reflection about thinking processes.

**Teach AI evaluation skills.** Students should learn to recognize when AI is wrong, biased, or oversimplified. This requires having enough knowledge to evaluate AI output—which circles back to foundational learning.

**Build in AI-free practice.** Some tasks should be completed without AI assistance, not as punishment but as skill development. Athletes don't use power tools during training. Students shouldn't use cognitive power tools when developing cognitive skills.

[INPUT NEEDED: How do you approach this balance at GGCS? What skills do you prioritize protecting from AI shortcuts?]

**Principles for AI without losing learning.** A framework for thinking through these decisions:

**Principle 1: Skills before shortcuts.** Students need foundational skills to use AI effectively. Don't let AI replace skill development—use it to enhance skills that are already established.

**Principle 2: Thinking before output.** The learning happens in the thinking process, not in the final product. Protect the thinking. AI can help with execution once thinking is done.

**Principle 3: Agency over automation.** Students should make decisions about their work, using AI as input rather than authority. The student remains in charge; AI assists.

**Principle 4: Metacognition always.** When students use AI, require reflection on how and why. This metacognitive layer preserves learning even when AI provides assistance.

**Principle 5: Verify, don't trust.** Students should evaluate AI output, not accept it uncritically. Building this verification habit protects against both AI errors and learned helplessness.

These principles don't ban AI. They ensure AI enhances learning rather than replacing it.

[INPUT NEEDED: What principles guide your thinking about AI and learning at GGCS? Would you add or modify any of these?]

---

### Your Input Needed for Post 9

1. **Student dependency observations:** Have you seen students become overly reliant on AI?
2. **Skill protection:** What skills do you prioritize protecting from AI shortcuts?
3. **Balance approach:** How do you think about the trade-off between efficiency and learning?
4. **Specific examples:** Cases where AI helped vs. hindered student thinking?

---

# POST 10: Leadership Visibility

## Title: "Why Your AI Strategy Can't Be Delegated to IT"

### Outline

**Hook:** The moment you realize AI isn't a technology issue

**The Meat:**
- Section 1: Why AI differs from previous technology adoption
- Section 2: The decisions only you can make
- Section 3: What visible leadership looks like in practice
- Section 4: Building your own AI fluency

**Bottom Line:** The leadership shift this moment requires

---

### Draft

I used to think AI was a technology issue. Something for IT to handle.

Then a teacher came to me with a question: "A student submitted work that reads exactly like ChatGPT, but they swear they wrote it themselves. What do I do?"

That wasn't an IT question. It was a question about academic integrity, student relationships, evidence standards, and school culture. It required leadership judgment, not technical expertise.

That's when I realized: AI decisions land on my desk whether I'm prepared for them or not.

[INPUT NEEDED: What was your "aha moment" about AI being a leadership issue? A specific situation that made this clear?]

**Why AI differs from previous technology.** Schools have navigated technology adoption before—computers, internet, mobile devices, learning management systems. We've developed playbooks for these transitions.

AI is different. Not because the technology is more complex (though it is), but because the decisions are different.

Previous technology mostly automated tasks: typing instead of handwriting, searching instead of going to the library, emailing instead of sending memos. The core human activities remained unchanged. AI doesn't just automate tasks—it performs cognitive work that we've traditionally considered distinctly human: writing, reasoning, problem-solving, creating.

This means AI touches the core of what education is about. When a machine can write a competent essay, what does "learning to write" mean? When a machine can solve problems step-by-step, what does "learning math" mean? These questions aren't technical. They're philosophical and pedagogical.

Your IT team can deploy tools and manage infrastructure. They can't decide what education should look like in an AI-enabled world. That's your job.

<Callout type="warning" title="The delegation trap">
When leaders delegate AI to IT, they often get technology solutions to human problems. AI policies that focus on tools rather than learning. Detection software that creates adversarial dynamics. Investments in technology without investments in training. The symptoms get treated; the underlying questions get ignored.
</Callout>

**The decisions only you can make.** Several AI decisions require leadership judgment, not technical expertise:

**What is the school's educational philosophy about AI?** Is AI a tool students should learn to use? A threat to authentic learning? Both? Neither? Your answer shapes everything else—and only you can provide it.

**How do we balance efficiency with learning?** AI can save significant time for teachers and students. But not all efficiency is beneficial. When does efficiency serve learning, and when does it shortcut it? This requires pedagogical judgment.

**What expectations do we communicate to families?** Parents need clarity about AI policies, and they need it from leadership. Your IT director can't speak for the school's educational vision.

**How do we handle integrity violations?** When AI use crosses lines, the response isn't technical—it's relational and cultural. What do we value? How do we restore trust? What consequences are appropriate?

**What does teacher support look like?** Teachers need training, time, and permission to experiment. Providing these requires resource allocation, scheduling decisions, and cultural leadership.

[INPUT NEEDED: What AI decisions have required your direct involvement at GGCS? What did you learn from handling them?]

**What visible leadership looks like.** Your engagement with AI sends signals throughout your school community. Here's what visible leadership looks like in practice:

**Use AI yourself.** You can't have informed opinions about AI in education without using it. Experiment with drafting communications, summarizing meeting notes, creating agendas. Learn what AI does well and poorly. Form your own views.

**Be present for AI training.** When teachers receive AI professional development, show up. Not to observe—to participate. This signals that AI matters enough for leadership engagement.

**Share your learning.** Talk openly about how you're using AI, what you're discovering, and where you're uncertain. Modeling learning gives permission for others to experiment.

**Make decisions visibly.** When AI questions arise, engage with them publicly. Explain your reasoning. Let the community see how you're thinking through these issues.

**Invite challenge.** You don't have all the answers. Neither does anyone else. Create space for teachers, students, and parents to push back, ask questions, and contribute ideas.

<BeforeAfter 
  before="AI is an IT initiative. Keep me informed of major issues."
  after="AI is a leadership priority. I'm engaged, visible, and learning alongside our community."
/>

**Building your own AI fluency.** You don't need to become a technical expert. You need to become an informed leader—someone who can ask good questions, evaluate options, and make sound judgments.

**Personal experimentation.** Commit to one AI task per day for a month. Not teaching others—just using it yourself. Drafting emails. Summarizing documents. Brainstorming ideas. Build intuition through experience.

**Diverse input.** Talk to people inside and outside education about how they're using AI. Talk to skeptics and enthusiasts. Read broadly. Attend sessions. Build a mental model from multiple perspectives.

**Structured reflection.** After experimenting, reflect: What did AI do well? Where did it fall short? What surprised me? What questions emerged? Turn experience into insight.

**Ongoing engagement.** AI is changing fast. Stay engaged. Follow developments. Update your thinking. Build learning into your leadership practice, not just a one-time initiative.

[INPUT NEEDED: How did you build your own AI fluency? What would you recommend to other school leaders?]

**The leadership shift.** This moment calls for a different kind of leadership than previous technology transitions demanded.

It requires intellectual engagement with deep questions about learning, skills, and human development.

It requires cultural leadership that shapes norms and expectations across a community.

It requires adaptive capacity to update your views as you learn and as the technology evolves.

It requires visible modeling that demonstrates the thoughtful engagement you're asking of others.

Your IT team has an important role. So do your teachers, your students, and your parents. But someone needs to hold the whole picture, make the hard calls, and lead the community through uncertainty.

That's your job.

[INPUT NEEDED: What leadership lessons have you learned from engaging with AI at GGCS? What would you tell other school leaders who are just beginning?]

---

### Your Input Needed for Post 10

1. **Your "aha moment":** When did you realize AI was a leadership issue, not a tech issue?
2. **Key decisions:** What AI decisions have required your direct involvement?
3. **Building fluency:** How did you develop your own AI understanding? What would you recommend?
4. **Leadership lessons:** What have you learned about leading through this transition?

---

# Summary: Input Needed Across All Posts

## Post 1: Academic Integrity
- Opening story of detection failure
- Patterns in why students use AI
- Assessment redesign experiments
- Specific assignment example

## Post 2: Teacher Training
- Training situation at GGCS
- PD approaches tried (what worked/didn't)
- Time savings examples
- Recommendations for other leaders

## Post 3: Deepfakes
- Direct experience with deepfake issues
- International/Indonesia context
- Digital citizenship approach
- Parent communication on this topic

## Post 4: AI Policy
- Policy creation story
- Specific frameworks/language
- Building buy-in process
- Your actual policy (excerpts)

## Post 5: Assessment Redesign
- Specific redesigned assignments
- Most practical principles
- "Bonus effect" evidence
- IB/PYP/MYP context

## Post 6: Parent Communication
- Actual parent questions
- Parent fears in your community
- Language that's worked
- Proactive communication done

## Post 7: AI Tutoring
- AI tutoring pitches received
- Evaluation process
- Direct experience with tools
- Skeptical questions you ask

## Post 8: Data Privacy
- Privacy concerns encountered
- Regulatory context (Indonesia/international)
- Vetting process
- Tools rejected for privacy reasons

## Post 9: Critical Thinking
- Student dependency observations
- Skills you protect from AI
- Balance approach
- Help vs. hinder examples

## Post 10: Leadership
- Your "aha moment"
- Key decisions requiring your involvement
- Building your own fluency
- Leadership lessons learned

---

# Next Steps

1. Review drafts and identify which posts resonate most
2. Provide input for those posts (can do 2-3 at a time)
3. I'll refine based on your input and create final MDX files
4. Review against SKILL.md checklist before publishing

Which posts do you want to tackle first?
