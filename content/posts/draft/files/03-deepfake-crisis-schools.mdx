---
title: "The Deepfake Crisis Schools Aren't Ready For"
description: "Reports of AI-generated child abuse imagery jumped from 4,700 to 440,000 in 18 months. Here's what school leaders need to know—and do."
date: "2026-01-24"
pillar: "practical-ai"
contentType: "evergreen"
readTime: "10 min read"
featured: true
interlinks:
  - "why-ai-skills-are-no-longer-optional"
---

In October 2024, over 44 girls in Des Moines were discovered to be victims of AI-generated explicit imagery. The perpetrators were their male classmates. The images were created using free apps that took seconds to use. The impact will last years.

This isn't an isolated incident. Reports of AI-generated child sexual abuse material (CSAM) to the National Center for Missing & Exploited Children jumped from 4,700 in 2023 to 440,000 in just the first six months of 2025. That's a 93-fold increase.

And most schools aren't ready for it.

{/* [INPUT NEEDED: Have you encountered any deepfake-related incidents, even rumors? How does this issue present in the Indonesian/international school context?] */}

## What's Actually Happening

The technology has become trivially easy to use. Apps that generate non-consensual intimate imagery from a normal photo are freely available and require no technical skill. Students are creating this content, sharing it, and using it for bullying, harassment, and worse.

The numbers are staggering. 13% of K-12 principals reported deepfake-related bullying incidents in the 2024-25 school year. 12% of students say they know of non-consensual intimate AI imagery circulating at their school. Some surveys put awareness of school-based deepfakes at 40-50%.

The victims are overwhelmingly female. The perpetrators are overwhelmingly male classmates.

A Louisiana 8th grader became one of the first students formally expelled for creating deepfake images of classmates under a newly-enacted state law. But most incidents don't result in expulsion. Many aren't reported at all. Victims are often too ashamed or afraid to come forward.

<Callout type="warning" title="The enforcement gap">
Over half of educators report receiving either no training or poor-quality training on recognizing and responding to AI-generated content. The policies haven't caught up to the technology.
</Callout>

## The Legal Landscape Is Shifting—Fast

Half of U.S. states enacted deepfake-related legislation in 2025. The federal TAKE IT DOWN Act, signed in May 2025, requires social media platforms to remove non-consensual intimate imagery within 48 hours of a report. It explicitly covers AI-generated content and provides criminal penalties for distribution.

But legislation alone doesn't protect students. Schools need to be able to respond before images spread, support victims effectively, and educate students about both the legal and human consequences of creating this content.

{/* [INPUT NEEDED: What's the legal/regulatory situation in Indonesia? Any relevant international school guidance?] */}

## Why Schools Are Unprepared

Schools are unprepared for three main reasons.

**Policies haven't caught up.** Many acceptable use policies and bullying policies were written before deepfakes became a student-accessible technology. They may not explicitly cover AI-generated content, creating ambiguity about consequences.

**Staff don't know what to look for.** Deepfake content is often shared through private channels—Snapchat, Discord servers, group texts—that adults don't see. By the time an incident surfaces, it may have spread widely.

**The emotional dimension is new.** Traditional harassment involves words or actions. Deepfake harassment creates permanent visual artifacts that can resurface indefinitely. The psychological impact is qualitatively different, and counselors may not have experience with it.

## What You Can Do Now

**Update your policies explicitly.** Your acceptable use policy, student code of conduct, and harassment policies should explicitly mention AI-generated content. Don't rely on general language—name the specific behavior. Make clear that creating, possessing, or distributing non-consensual intimate imagery, including AI-generated imagery, is a serious violation with serious consequences.

**Train your staff.** Teachers need to recognize warning signs: students clustered around phones and dispersing when adults approach, sudden social targeting of particular students, references to images or apps in overheard conversations. Counselors need protocols for supporting victims of image-based abuse.

**Educate your students.** Students need to understand both the legal consequences (potential criminal charges) and the human impact (lasting trauma to victims). The SHIELD framework provides a useful structure: See it, don't share it. Hold those responsible accountable. Investigate and report. Empower bystanders. Lead with empathy. Document and preserve evidence.

<Callout type="tip" title="The bystander angle">
Research suggests students are more likely to intervene when they understand they're not powerless. Teaching students what TO do (report, support the victim, don't share) is more effective than just teaching what NOT to do.
</Callout>

**Build response protocols.** When (not if) an incident occurs, you need clear procedures: who is notified, how evidence is preserved, how the victim is supported, how the perpetrator is held accountable, how families are informed, and how you balance privacy concerns with community communication.

{/* [INPUT NEEDED: What's your digital citizenship approach at GGCS? Any specific programs or frameworks you use?] */}

## A Difficult Conversation Worth Having

This isn't a comfortable topic. It involves talking to students about sexual imagery, consent, and technology in ways that may feel inappropriate for younger grades and awkward for all ages.

But discomfort doesn't make it optional. Students are encountering this technology whether schools address it or not. The question is whether they encounter it with guidance or without.

For elementary students, this can start with consent and digital citizenship basics—you don't share pictures of people without permission, you don't use technology to hurt others.

For middle and high school students, the conversation needs to be more direct: what deepfakes are, why creating them is harmful, what the legal consequences are, and what to do if you encounter them.

The schools that protect students best will be the ones that talk about this openly—not the ones that hope it doesn't happen to them.

---

## References

1. [AI and deepfake-related bullying](https://www.rand.org/pubs/research_reports/RRA956-22.html) - RAND Corporation
2. [TAKE IT DOWN Act](https://www.congress.gov/bill/119th-congress/senate-bill/146) - U.S. Congress
3. [Reports of AI CSAM to NCMEC](https://www.nytimes.com/2025/01/04/technology/ai-child-sexual-abuse.html) - New York Times
4. [Louisiana expulsion case](https://www.theadvocate.com/baton_rouge/news/education/louisiana-student-expelled-deepfake-ai-images/article_5f8c3d7a-8e4f-11ef-9a4c-6b6a3f1c9e2d.html) - The Advocate
