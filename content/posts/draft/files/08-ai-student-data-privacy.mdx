---
title: "AI and Student Data: The Questions Every School Leader Should Ask"
description: "Where does student data go when teachers and students use AI tools? Here's what you need to know about privacy, compliance, and vetting AI vendors."
date: "2026-02-05"
pillar: "practical-ai"
contentType: "evergreen"
readTime: "9 min read"
featured: false
interlinks:
  - "why-ai-skills-are-no-longer-optional"
---

When a student enters a prompt into ChatGPT, where does that data go? When a teacher uses an AI tool to grade essays, who sees the student work? When your school adopts an AI platform, what happens to the learning data it collects?

These questions matter. And at most schools, nobody's asking them carefully enough.

Only 10% of institutions have formal AI use guidelines, which means 90% are allowing AI tools into classrooms without clear frameworks for data governance. The regulatory landscape is evolving—but regulations only work if schools understand and enforce them.

{/* [INPUT NEEDED: What privacy concerns have you encountered at GGCS? Any specific situations that made you think more carefully about this?] */}

## What AI Tools Actually Collect

AI tools collect more than you might think. When students use AI for learning, the systems typically capture conversation histories (every prompt and response), behavioral patterns (time spent, topics explored, error patterns), submitted work (essays, problems, drafts), and learning indicators (what students struggle with, how they progress).

When teachers use AI, the tools may capture student information embedded in prompts ("give me feedback on this essay by John Smith"), grade data used to train feedback systems, and communication content (parent emails, report card comments).

This data is valuable—to the school for improving instruction, but also to AI companies for improving their models. The question is who controls it and how it's protected.

## The Legal Landscape

Several regulations govern student data in AI contexts.

**FERPA** (in the U.S.) protects student education records. Schools must ensure AI vendors handling student data are properly designated as "school officials" with legitimate educational interests. Without proper contracts, using AI tools with student data may violate FERPA.

**COPPA** regulates data collection from children under 13. The 2025 amendments shifted from opt-out to opt-in consent requirements—parents must actively agree before their children's data can be collected. This affects AI tools used in elementary schools significantly.

**State laws** vary widely. Twenty-one states list data security as a focus area in their AI guidance. Some have specific requirements about data localization, retention limits, or breach notification that affect AI tool adoption.

<Callout type="warning" title="The free tool trap">
Free AI tools often "pay" through data collection. If a tool is free, the product might be your students' data. Understand the business model before adopting any AI tool—especially free ones.
</Callout>

{/* [INPUT NEEDED: What's the regulatory situation in Indonesia? Any specific international school considerations?] */}

## Questions for Vetting AI Vendors

Before adopting any AI tool, ask these questions:

**What data do you collect?** Get specific. Conversation histories? Learning behaviors? Submitted work? Understand the full scope, not just what's necessary for the tool to function.

**Where is data stored, and for how long?** Data stored outside your jurisdiction may not be protected by your local laws. Data retained indefinitely presents ongoing risk. Look for clear retention policies and data localization options.

**Who has access to the data?** Internal staff? Third-party vendors? Researchers? AI model trainers? The chain of access should be explicit and limited.

**How is data secured?** What encryption is used? What access controls? What happens in a breach? Don't accept "we take security seriously" as an answer—ask for specifics.

**Is the tool compliant with relevant regulations?** Ask for compliance documentation, not just assertions. FERPA school official agreements, COPPA verifiable parental consent mechanisms, and relevant state/international compliance.

**What happens to data if we stop using the tool?** Can data be exported? Is it deleted? How long does that process take? You need an exit strategy.

<BeforeAfter 
  before="This AI tool looks helpful—let's try it with students."
  after="Let's review the privacy policy, verify FERPA compliance, understand the data practices, and get appropriate consent before piloting with students."
/>

## A Practical Vetting Process

**Step 1: Review before use.** No AI tool goes into classrooms before someone reviews its data practices. This doesn't have to be exhaustive for every tool, but it needs to happen.

**Step 2: Categorize by risk.** Tools that collect no student data (teacher-only use with anonymized content) are lower risk. Tools that collect identifiable student data require more scrutiny.

**Step 3: Require documentation.** For higher-risk tools, require vendors to provide written privacy policies, data processing agreements, and compliance certifications.

**Step 4: Get appropriate consent.** Depending on the tool and your jurisdiction, you may need parent notification, opt-in consent, or explicit agreements.

**Step 5: Review periodically.** AI tools update constantly. Data practices that were acceptable at adoption may change. Build in periodic reviews.

<Callout type="tip" title="The approved tool list">
Maintain a list of vetted and approved AI tools. Teachers can use tools on the list without additional review. New tools require going through the vetting process. This balances innovation with protection.
</Callout>

{/* [INPUT NEEDED: What vetting process do you use at GGCS? Any tools you've rejected based on privacy concerns?] */}

## When to Say No

Some AI tools shouldn't be used with students regardless of how educationally promising they seem.

**No clear data policies.** If a vendor can't or won't explain their data practices, they don't earn access to your students.

**Data used for model training.** If student inputs are used to train AI models, students are providing free labor and exposing their data to unknown future uses. This should be a dealbreaker unless clearly disclosed and consented to.

**Indefinite retention without justification.** AI tools should retain data only as long as necessary for the educational purpose. "We keep everything forever" is a red flag.

**Weak security practices.** No encryption, unclear access controls, or a history of breaches should disqualify a vendor.

**Jurisdiction concerns.** If data is stored in jurisdictions with weak privacy protections or government access rights that concern you, reconsider.

## Building a Privacy-Aware Culture

This isn't just about policies—it's about building awareness among teachers and students.

**Teach teachers to think before prompting.** Don't include student names or identifying information in AI prompts unless necessary. Anonymize when possible. Think about what data you're creating.

**Teach students data awareness.** What you type into AI tools may be stored, analyzed, and used in ways you don't expect. Think before you share personal information.

**Make privacy part of technology decisions.** Every conversation about adopting new tools should include "what are the data implications?" as a standard question.

---

## References

1. [AI in Education Statistics](https://www.demandsage.com/ai-in-education-statistics/) - DemandSage
2. [COPPA 2025 Amendments](https://www.ftc.gov/legal-library/browse/rules/childrens-online-privacy-protection-rule-coppa) - FTC
3. [State AI Education Guidance](https://www.cste.org/general/custom.asp?page=AIEduPolicy) - CSTE
4. [Student Data Privacy Consortium](https://privacy.a4l.org/) - A4L Community
