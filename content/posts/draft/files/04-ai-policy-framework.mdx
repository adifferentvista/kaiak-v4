---
title: "Your School Needs an AI Policy. Here's a Framework That Actually Works."
description: "90% of schools have no formal AI guidelines. Generic bans don't work. Here's a framework for policies that provide clarity without rigidity."
date: "2026-01-27"
pillar: "practical-ai"
contentType: "evergreen"
readTime: "10 min read"
featured: false
interlinks:
  - "why-ai-skills-are-no-longer-optional"
---

Only 10% of schools and universities have formal AI use guidelines. The other 90% are operating in a policy vacuum—which means teachers make it up as they go, students guess at boundaries, and parents receive inconsistent messages.

That vacuum creates real problems. Without clear guidelines, a student might use AI appropriately in one class and violate unspoken rules in another. Teachers spend energy reinventing the wheel instead of teaching. Integrity violations become "I didn't know" situations instead of clear-cut cases.

But the solution isn't a 50-page policy document that tries to anticipate every scenario. It's a framework that provides clarity where it matters while remaining flexible enough to adapt.

{/* [INPUT NEEDED: What was the policy situation when you started at GGCS? What prompted you to create policy?] */}

## Why Generic Policies Fail

"Students may not use AI" doesn't work. It's unenforceable, too broad, and treats a nuanced tool as a binary. Some AI uses clearly support learning (checking grammar, exploring ideas, getting feedback). Others clearly undermine it (having AI write entire assignments). A blanket ban can't distinguish between them.

"Students may use AI with teacher permission" is better but creates inconsistency. One teacher allows it; another doesn't. Students face a maze of varying expectations. The cognitive load of tracking different rules by class interferes with learning.

"Use AI responsibly" is too vague to be actionable. What does "responsible" mean? Without specifics, it provides no guidance for edge cases—which is where guidance is most needed.

<Callout type="warning" title="The enforcement problem">
Any policy you can't consistently enforce isn't really a policy—it's a suggestion. And selective enforcement breeds cynicism. If some students get caught and others don't, the policy loses legitimacy.
</Callout>

## A Framework That Works: Three Domains

Effective AI policies address three distinct domains, each with different considerations.

**Student use** is about learning. The questions here are educational: Does this AI use support or undermine the learning objective? Is the student developing skills or outsourcing them? Can the student demonstrate understanding independent of AI assistance?

**Teacher use** is about effectiveness and ethics. Teachers might use AI for lesson planning, communication, assessment creation, or feedback. The questions are: Does this use save time while maintaining quality? Does it protect student privacy? Does it model responsible use?

**Institutional use** is about data, equity, and resource allocation. When the school adopts AI tools, the questions expand: What data is collected? Who has access? Does this create or reduce inequities? What training and support are needed?

Most schools focus only on student use. But teacher and institutional use require attention too.

{/* [INPUT NEEDED: How did you structure your GGCS policy? Did you address all three domains?] */}

## Decision-Making Frameworks Over Exhaustive Rules

Rather than trying to enumerate every allowed and prohibited use, effective policies provide frameworks for making decisions.

**The AI Transparency Spectrum** helps classify different uses. At one end: AI use that must be disclosed and documented (significant assistance with drafts, analysis, or creative work). In the middle: AI use that's encouraged and normalized (grammar checking, brainstorming, formatting). At the other end: AI use that's prohibited (submitting AI-generated work as original without disclosure).

**The Purpose Test** asks: Is the AI helping the student learn, or is it doing the learning for the student? If a student uses AI to understand a concept they're struggling with, that's learning. If they use AI to produce an assignment without engaging with the material, that's substitution.

**The Dependency Check** asks: Can the student do this without AI? If not, they need to develop the underlying skill before leveraging AI to enhance it. You don't skip learning to drive because autonomous vehicles exist.

<BeforeAfter 
  before="AI is prohibited except when explicitly allowed by the teacher."
  after="AI use should be transparent, purposeful, and documented. Use the Purpose Test: Is AI helping you learn, or doing the learning for you?"
/>

## Building Buy-In

A policy nobody follows isn't a policy. Getting buy-in requires involving stakeholders in development—not just announcing decisions from above.

**With teachers:** Start with their pain points. What AI questions are they struggling to answer? What inconsistencies are they experiencing? Let the policy solve real problems they face. Give them ownership over implementation in their classrooms.

**With students:** Be honest about why the policy exists. Students accept rules better when they understand the reasoning. "We want you to develop skills that AI can't replace" lands better than "Because we said so." Ask for their input—they often know more about AI use than adults.

**With parents:** Proactive communication beats reactive damage control. Share your policy before problems arise. Explain your reasoning. Invite questions. Parents who understand your approach are allies; parents who are surprised become critics.

{/* [INPUT NEEDED: How did you build buy-in at GGCS? What worked? What resistance did you face?] */}

## A Starting Template

Here's a framework you can adapt. It won't fit every school, but it's a starting point.

**General Principle:** AI tools are part of modern learning. Our goal is to teach students to use them responsibly, not to pretend they don't exist. We expect transparency about AI use, purposeful application that supports learning, and continued development of foundational skills.

**For Students:** You may use AI tools unless an assignment specifically prohibits them. When you use AI significantly (beyond grammar checking or basic formatting), document your use: what tool, what prompts, how you used the output. Ask yourself: Am I learning, or am I outsourcing? If an assignment requires demonstrating your own understanding, AI assistance may not be appropriate.

**For Teachers:** You set AI expectations for your assignments. Communicate these expectations clearly. Consider requiring process documentation rather than only final products. Model responsible AI use yourself. If you use AI to provide feedback, let students know.

**For the Institution:** We evaluate AI tools for privacy compliance, educational value, and equity implications before adoption. We provide ongoing professional development. We review this policy annually and adapt as the technology evolves.

{/* [INPUT NEEDED: Would you share actual language from your GGCS policy?] */}

---

## References

1. [AI in Education Statistics](https://www.demandsage.com/ai-in-education-statistics/) - DemandSage
2. [State AI guidance survey](https://www.cste.org/general/custom.asp?page=AIEduPolicy) - CSTE
3. [UNESCO AI in Education Framework](https://www.unesco.org/en/digital-education/artificial-intelligence) - UNESCO
4. [K-12 AI Guidelines](https://www.cosn.org/ai) - CoSN
