---
title: "The Deepfake Crisis Schools Aren't Ready For"
description: "AI-generated abuse imagery reports jumped from 4,700 to 440,000 in 18 months. 13% of principals report deepfake bullying incidents. Here's what school leaders must do now."
date: "2026-01-20"
pillar: "practical-ai"
contentType: "evergreen"
readTime: "10 min read"
featured: true
keywords: ["deepfakes schools", "AI-generated imagery students", "deepfake bullying", "student safety AI", "TAKE IT DOWN Act schools"]
interlinks:
  - "why-ai-skills-are-no-longer-optional"
  - "ai-policy-framework"
---

Reports of AI-generated child sexual abuse material jumped from 4,700 in 2023 to 440,000 in the first six months of 2025—a 93-fold increase. 13% of K-12 principals reported deepfake-related bullying incidents in 2024-25. Most schools are unprepared: over half of educators have received no training on recognizing or responding to AI-generated content. Schools need updated policies, trained staff, and clear response protocols now.

In October 2024, over 44 girls in Des Moines were discovered to be victims of AI-generated explicit imagery created by male classmates using free apps. The images took seconds to create. The impact will last years.

{/* [INPUT NEEDED: Have you encountered any deepfake-related incidents, even rumors? How does this issue present in the Indonesian/international school context?] */}

## How Bad Is the Deepfake Problem in Schools?

The numbers document a crisis:

| Metric | Data | Source |
|--------|------|--------|
| AI CSAM reports to NCMEC | 4,700 (2023) → 440,000 (first half 2025) | New York Times |
| Principals reporting deepfake bullying | 13% in 2024-25 | RAND Corporation |
| Students aware of deepfakes at their school | 40-50% | Multiple surveys |
| Educators with no/poor deepfake training | Over 50% | RAND Corporation |

The technology has become trivially easy. Apps generating non-consensual intimate imagery from normal photos are freely available and require no technical skill. Students are creating, sharing, and weaponizing this content for bullying and harassment.

The victims are overwhelmingly female. The perpetrators are overwhelmingly male classmates.

A Louisiana 8th grader became one of the first students formally expelled for creating deepfake images of classmates under newly-enacted state law. But most incidents don't result in expulsion. Many aren't reported at all—victims are often too ashamed or afraid to come forward.

<Callout type="warning" title="The enforcement gap">
Over half of educators report receiving either no training or poor-quality training on AI-generated content. Policies haven't caught up to technology students are already using.
</Callout>

## What Laws Now Apply to School Deepfake Incidents?

The legal landscape shifted dramatically in 2025:

**Federal law:** The TAKE IT DOWN Act (signed May 2025) requires social media platforms to remove non-consensual intimate imagery within 48 hours of a report. It explicitly covers AI-generated content and provides criminal penalties for distribution.

**State laws:** Half of U.S. states enacted deepfake-related legislation in 2025, with varying approaches to criminal penalties, civil liability, and school requirements.

**School implications:** Creating, possessing, or distributing non-consensual intimate imagery—including AI-generated imagery—can result in criminal charges for students, not just school discipline.

But legislation alone doesn't protect students. Schools must respond before images spread, support victims effectively, and educate students about both legal and human consequences.

{/* [INPUT NEEDED: What's the legal/regulatory situation in Indonesia? Any relevant international school guidance?] */}

## Why Aren't Schools Ready for This?

Three gaps leave schools vulnerable:

**Policy gaps.** Acceptable use policies and bullying policies written before deepfakes became student-accessible may not explicitly cover AI-generated content, creating ambiguity about consequences.

**Training gaps.** Deepfake content spreads through private channels—Snapchat, Discord, group texts—that adults don't see. Staff don't know warning signs to watch for.

**Support gaps.** Traditional harassment involves words or actions. Deepfake harassment creates permanent visual artifacts that can resurface indefinitely. The psychological impact is qualitatively different, and counselors may lack experience with image-based abuse.

## What Should Schools Do Right Now?

Four immediate actions:

**1. Update policies explicitly.** Your acceptable use policy, student code of conduct, and harassment policies should explicitly mention AI-generated content. Name the specific behavior: "Creating, possessing, or distributing non-consensual intimate imagery, including AI-generated imagery, is a serious violation with serious consequences."

**2. Train staff to recognize warning signs.** Students clustered around phones who disperse when adults approach. Sudden social targeting of particular students. References to images or apps in overheard conversations. Counselors need protocols for supporting victims of image-based abuse.

**3. Educate students using the SHIELD framework:**
- **S**ee it, don't share it
- **H**old those responsible accountable
- **I**nvestigate and report
- **E**mpower bystanders
- **L**ead with empathy
- **D**ocument and preserve evidence

**4. Build response protocols.** When (not if) an incident occurs, you need clear procedures: who is notified, how evidence is preserved, how the victim is supported, how the perpetrator is held accountable, how families are informed, and how you balance privacy with community communication.

<Callout type="tip" title="The bystander angle works">
Research shows students are more likely to intervene when they understand they're not powerless. Teaching what TO do (report, support the victim, don't share) is more effective than just teaching what NOT to do.
</Callout>

{/* [INPUT NEEDED: What's your digital citizenship approach at GGCS? Any specific programs or frameworks you use?] */}

## How Do You Talk to Students About This?

This isn't comfortable. It involves discussing sexual imagery, consent, and technology in ways that feel inappropriate for younger grades and awkward for all ages.

But discomfort doesn't make it optional. Students encounter this technology whether schools address it or not. The question is whether they encounter it with guidance.

**For elementary students:** Start with consent and digital citizenship basics. You don't share pictures of people without permission. You don't use technology to hurt others.

**For middle and high school students:** Be direct. What deepfakes are. Why creating them causes lasting harm. What the legal consequences are (including potential criminal charges). What to do if you encounter them.

The schools that protect students best talk about this openly—they don't hope it won't happen to them.

---

## Frequently Asked Questions

### Can students really face criminal charges for creating deepfakes?

Yes. Under the TAKE IT DOWN Act and various state laws, creating or distributing non-consensual intimate imagery—including AI-generated imagery—can result in criminal charges. Minors aren't exempt, though consequences vary by jurisdiction and age.

### What if the deepfake doesn't show nudity?

Non-sexual deepfakes can still constitute harassment, defamation, or bullying depending on content and context. Even "prank" deepfakes that embarrass or mock students may violate school policies and potentially laws.

### How do we investigate when images are on private platforms we can't access?

Focus on what you can verify: witness statements, device confiscation (following proper procedures), and reports from victims or bystanders. You don't need to see the images to respond to credible reports of their existence.

### Should we involve law enforcement?

For any imagery depicting minors in sexual situations—including AI-generated imagery—consult with law enforcement. Many jurisdictions require reporting suspected CSAM. Document your decision-making process.

### What support do deepfake victims need?

Immediate emotional support, assurance that they're not at fault, practical help removing content (using TAKE IT DOWN Act provisions), long-term counseling access, and protection from retaliation. The psychological impact of image-based abuse can be severe and lasting.

---

## References

1. [AI and deepfake-related bullying](https://www.rand.org/pubs/research_reports/RRA956-22.html) - RAND Corporation
2. [TAKE IT DOWN Act](https://www.congress.gov/bill/119th-congress/senate-bill/146) - U.S. Congress
3. [Reports of AI CSAM to NCMEC](https://www.nytimes.com/2025/01/04/technology/ai-child-sexual-abuse.html) - New York Times
4. [Louisiana expulsion case](https://www.theadvocate.com/baton_rouge/news/education/louisiana-student-expelled-deepfake-ai-images/article_5f8c3d7a-8e4f-11ef-9a4c-6b6a3f1c9e2d.html) - The Advocate
