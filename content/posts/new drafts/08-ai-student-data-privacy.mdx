---
title: "AI and Student Data: The Questions Every School Leader Should Ask"
description: "90% of schools have no AI data governance framework. COPPA 2025 requires opt-in consent for under-13s. Here's what AI tools collect, what laws apply, and the 6 questions to ask every vendor."
date: "2026-02-05"
pillar: "practical-ai"
contentType: "evergreen"
readTime: "9 min read"
featured: false
keywords: ["student data privacy AI", "AI tools FERPA", "COPPA AI schools", "AI vendor vetting schools"]
interlinks:
  - "why-ai-skills-are-no-longer-optional"
  - "ai-policy-framework"
---

90% of schools have no formal AI data governance framework—while AI tools collect conversation histories, learning behaviors, submitted work, and error patterns. The 2025 COPPA amendments require opt-in parental consent for children under 13. Free AI tools often "pay" through data collection. Here are the six questions every school leader should ask before any AI tool touches student data—and the red flags that mean saying no.

{/* [INPUT NEEDED: What privacy concerns have you encountered at GGCS? Any specific situations that made you think more carefully about this?] */}

## What Data Do AI Tools Actually Collect?

AI tools collect more than most educators realize:

| Data Type | Student Use | Teacher Use |
|-----------|-------------|-------------|
| **Conversation histories** | Every prompt and response | Student info embedded in prompts |
| **Behavioral patterns** | Time spent, topics explored, error patterns | Usage patterns, workflow habits |
| **Submitted work** | Essays, problems, drafts | Student work being graded/reviewed |
| **Learning indicators** | Struggles, progress, misconceptions | Performance data for feedback systems |
| **Communication content** | N/A | Parent emails, report card comments |

This data is valuable—to schools for improving instruction, but also to AI companies for training models. The question: who controls it and how is it protected?

## What Laws Govern Student Data in AI Tools?

Three regulatory frameworks matter most:

**FERPA (U.S.)** protects student education records. Schools must ensure AI vendors are properly designated as "school officials" with legitimate educational interests. Without proper contracts, using AI tools with student data may violate FERPA.

**COPPA (2025 amendments)** regulates data from children under 13. The critical change: shifted from opt-out to **opt-in consent**—parents must actively agree before children's data can be collected. This significantly affects AI tools in elementary schools.

**State laws** vary widely. Twenty-one states list data security as a focus area in their AI guidance. Some require data localization, retention limits, or specific breach notification protocols.

<Callout type="warning" title="The free tool trap">
If an AI tool is free, the product might be your students' data. Free tools often monetize through data collection—for model training, advertising, or sale to third parties. Understand the business model before adopting any AI tool.
</Callout>

{/* [INPUT NEEDED: What's the regulatory situation in Indonesia? Any specific international school considerations?] */}

## What Six Questions Should You Ask Every AI Vendor?

Before adopting any AI tool that touches student data:

| Question | Why It Matters | Red Flag Answer |
|----------|---------------|-----------------|
| **What data do you collect?** | Understand full scope, not just minimum | "Whatever is needed for the tool to function" |
| **Where is data stored, and for how long?** | Jurisdiction affects legal protections | "Indefinitely" or "various global locations" |
| **Who has access to the data?** | Chain of access should be explicit and limited | "Our partners" without specifics |
| **How is data secured?** | Encryption, access controls, breach protocols | "We take security seriously" without details |
| **Is the tool compliant with FERPA/COPPA?** | Require documentation, not assertions | "We believe so" or no documentation |
| **What happens to data if we stop using the tool?** | You need an exit strategy | "Data is retained" or unclear process |

<BeforeAfter 
  before="This AI tool looks helpful—let's try it with students."
  after="Let's review the privacy policy, verify FERPA compliance, understand data practices, and get appropriate consent before piloting."
/>

## What Does a Practical Vetting Process Look Like?

**Step 1: Review before use.** No AI tool goes into classrooms before someone reviews its data practices. This doesn't have to be exhaustive for every tool, but it must happen.

**Step 2: Categorize by risk.** Tools that collect no student data (teacher-only use with anonymized content) are lower risk. Tools collecting identifiable student data require more scrutiny.

**Step 3: Require documentation.** For higher-risk tools: written privacy policies, data processing agreements, and compliance certifications.

**Step 4: Get appropriate consent.** Depending on tool and jurisdiction: parent notification, opt-in consent, or explicit agreements. Under-13 tools require COPPA-compliant consent.

**Step 5: Review periodically.** AI tools update constantly. Data practices acceptable at adoption may change. Build in annual reviews.

<Callout type="tip" title="The approved tool list">
Maintain a list of vetted and approved AI tools. Teachers can use listed tools without additional review. New tools require the vetting process. This balances innovation with protection.
</Callout>

{/* [INPUT NEEDED: What vetting process do you use at GGCS? Any tools you've rejected based on privacy concerns?] */}

## When Should You Say No to an AI Tool?

Some AI tools shouldn't be used with students regardless of educational promise:

**No clear data policies.** If a vendor can't or won't explain their data practices, they don't earn access to your students.

**Data used for model training without consent.** If student inputs train AI models, students provide free labor and expose data to unknown future uses. Dealbreaker unless clearly disclosed and consented to.

**Indefinite retention without justification.** AI tools should retain data only as long as educationally necessary. "We keep everything forever" is a red flag.

**Weak security practices.** No encryption, unclear access controls, or breach history should disqualify a vendor.

**Concerning jurisdictions.** Data stored in jurisdictions with weak privacy protections or government access rights that concern you warrants reconsideration.

## How Do You Build a Privacy-Aware Culture?

This isn't just about policies—it's about building awareness:

**Teach teachers to think before prompting.** Don't include student names or identifying information unless necessary. Anonymize when possible. Think about what data you're creating.

**Teach students data awareness.** What you type into AI tools may be stored, analyzed, and used in unexpected ways. Think before sharing personal information.

**Make privacy part of every technology decision.** Every conversation about adopting new tools should include "what are the data implications?" as a standard question.

---

## Frequently Asked Questions

### Can teachers use ChatGPT with student work?

Depends on your school's agreement with OpenAI and what data is shared. Including student names, identifying information, or original student work in prompts creates FERPA implications. Safer: anonymize work before using AI for feedback.

### What about AI tools that say they don't train on school data?

Get it in writing. Verify the claim applies to your specific agreement, not just their marketing. Some tools have different terms for free vs. paid tiers.

### How do we handle AI tools students use at home?

You can't control home use, but you can educate students about data implications and avoid requiring use of tools with problematic data practices for homework.

### Is Google/Microsoft AI safer because we already have contracts?

Possibly—but AI features may have different data practices than the core tools. Verify that your existing data processing agreements cover the AI functionality.

### What if parents refuse consent for AI tools?

Provide alternatives that don't require AI. Students shouldn't be penalized for their parents' privacy concerns. This may require more teacher time but respects family choices.

---

## References

1. [AI in Education Statistics](https://www.demandsage.com/ai-in-education-statistics/) - DemandSage
2. [COPPA 2025 Amendments](https://www.ftc.gov/legal-library/browse/rules/childrens-online-privacy-protection-rule-coppa) - FTC
3. [State AI Education Guidance](https://www.cste.org/general/custom.asp?page=AIEduPolicy) - CSTE
4. [Student Data Privacy Consortium](https://privacy.a4l.org/) - A4L Community
